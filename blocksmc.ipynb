{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099a32c0",
   "metadata": {},
   "source": [
    "#### Load libraries and set CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba19ffa-c34a-4e8f-bfa4-637e52cf1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Poisson, Normal, Uniform, Distribution, Categorical\n",
    "from distributions import TruncatedDiagonalMVN\n",
    "\n",
    "import numpy as np\n",
    "import sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58c0eb-744a-4ae2-b6e2-83eaefb94051",
   "metadata": {},
   "source": [
    "#### Image characteristics and point spread function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 1000                               # the number of images in our dataset\n",
    "max_objects_generated = 10\n",
    "D = max_objects_generated + 1                   # there are (max_objects_generated + 1) possible source counts for each image: {0,1,2,...,max_objects_generated}\n",
    "img_dim = 15                                    # the height and width of our images\n",
    "eta = 3.25                                      # PSF variance\n",
    "H = img_dim                                     # height of images\n",
    "W = img_dim                                     # width of images\n",
    "min_flux = torch.tensor(6400., device = device) # minimum flux\n",
    "background_intensity = 3 * min_flux             # background intensity of images\n",
    "\n",
    "def psf(H, W, D, u_h, u_w, eta):\n",
    "    psf_marginal_H = 1 + torch.arange(H, dtype=torch.float32, device = device)\n",
    "    psf_marginal_W = 1 + torch.arange(W, dtype=torch.float32, device = device)\n",
    "    \n",
    "    psf = ((-(psf_marginal_H.view(1, H, 1, 1) - u_h.view(1, 1, D, -1))**2 - (psf_marginal_W.view(W, 1, 1, 1) - u_w.view(1, 1, D, -1))**2)/(2*eta**2)).exp()\n",
    "    psf = psf/psf.sum([0,1]).view(1, 1, D, -1)\n",
    "    \n",
    "    return psf.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c2af3-beaf-4ade-9caf-5b48d4208acf",
   "metadata": {},
   "source": [
    "#### Generate synthetic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc65334",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Priors for number of objects, fluxes, locations\n",
    "s_prior = Categorical((1/D)*torch.ones(D, device = device))\n",
    "flux_prior = Normal(10 * min_flux, 2 * min_flux)\n",
    "u_prior = Uniform(torch.zeros(2, device = device), torch.tensor((H,W), device = device))\n",
    "\n",
    "# Create tensors to store data for multiple images\n",
    "s = torch.zeros(num_images, device=device)\n",
    "flux = torch.zeros(num_images, D, device=device)\n",
    "u = torch.zeros(num_images, D, 2, device=device)\n",
    "true_intensity = torch.zeros(num_images, H, W, device=device)\n",
    "images = torch.zeros(num_images, H, W, device=device)\n",
    "observed_flux = torch.zeros(num_images, device=device)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_images):\n",
    "    # Sample number of objects, fluxes, locations\n",
    "    s[i] = s_prior.sample()\n",
    "    s_indicator = torch.logical_and(torch.arange(D, device = device) <= s[i],\n",
    "                                    torch.arange(D, device = device) > torch.zeros(1, device=device))\n",
    "    flux[i] = flux_prior.sample([D]) * s_indicator\n",
    "    u[i] = u_prior.sample([D]) * s_indicator.unsqueeze(1)\n",
    "\n",
    "    # Compute true intensity of image\n",
    "    star_intensity = (flux[i].view(1, 1, D) * psf(H, W, D, u[i][:,0], u[i][:,1], eta)).sum(2)\n",
    "    true_intensity[i] = background_intensity + star_intensity\n",
    "\n",
    "    # Sample image\n",
    "    images[i] = Poisson(true_intensity[i]).sample().to(device)\n",
    "\n",
    "    # Compute observed flux\n",
    "    observed_flux[i] = (images[i] - background_intensity).sum([0,1])\n",
    "    \n",
    "    print(f\"image {i+1}\\n\", \"s\\n\", s[i], \"\\n\\ntotal flux\\n\", flux[i].sum(), \"\\n\\nu\\n\", u[i], \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76ac17",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241a60e",
   "metadata": {},
   "source": [
    "#### SMC-Deblender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempered_log_p_x_given_z(image, flux, u, tempering_factor):\n",
    "    num_blocks = flux.size(0)\n",
    "    num_particles = flux.size(1)\n",
    "    \n",
    "    rate = (psf(H, W, num_blocks, u[:,:,0], u[:,:,1], eta) * flux.view(1, 1, num_blocks, -1)).sum(2) + background_intensity\n",
    "    cond_ll = Poisson(rate).log_prob(image.view(img_dim, img_dim, 1)).sum([0,1])\n",
    "    tempered_cond_ll = tempering_factor.unsqueeze(1) * torch.stack(torch.split(cond_ll, num_particles//num_blocks, dim=0), dim=0)\n",
    "\n",
    "    return tempered_cond_ll\n",
    "\n",
    "\n",
    "\n",
    "def log_target(image, s, flux, u, tempering_factor):\n",
    "    num_blocks = flux.size(0)\n",
    "    num_particles = flux.size(1)\n",
    "    \n",
    "    s_indicator = torch.logical_and(torch.arange(num_blocks, device = device).view(num_blocks,1) <= s,\n",
    "                                    torch.arange(num_blocks, device = device).view(num_blocks,1) > torch.zeros(num_particles, device=device))\n",
    "\n",
    "    log_targ = (flux_prior.log_prob(flux) * s_indicator).sum(0)   # s_prior.log_prob(s) can be omitted; it appears in both the numerator and denominator of the MH acceptance ratio with the same s\n",
    "    log_targ += (u_prior.log_prob(u) * s_indicator.unsqueeze(2)).sum(2).sum(0)\n",
    "    log_targ += tempered_log_p_x_given_z(image, flux, u, tempering_factor).flatten(0)\n",
    "    \n",
    "    return log_targ\n",
    "\n",
    "\n",
    "\n",
    "def MCMC_kernel(image, s_tminus1, s_t, flux_tminus1, u_tminus1, tau_tminus1):\n",
    "    num_blocks = flux_tminus1.size(0)\n",
    "    num_particles = flux_tminus1.size(1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    s_indicator = torch.logical_and(torch.arange(num_blocks, device = device).view(num_blocks,1) <= s_tminus1,\n",
    "                                    torch.arange(num_blocks, device = device).view(num_blocks,1) > torch.zeros(num_particles, device=device))\n",
    "    \n",
    "    \n",
    "    \n",
    "    flux_prev = flux_tminus1\n",
    "    u_prev = u_tminus1\n",
    "    s_prev = s_tminus1\n",
    "    s_new = s_t\n",
    "    \n",
    "\n",
    "\n",
    "    flux_sd_scale = 2500\n",
    "    flux_proposal_sd = flux_sd_scale*torch.ones(1, device=device)\n",
    "    \n",
    "    u_sd_scale = 0.25\n",
    "    u_proposal_sd = u_sd_scale*torch.ones(1, device=device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    flux_sq_jump_dist_prev = torch.tensor(1e-10, device=device)\n",
    "    flux_rel_sq_jump_dist_tol = 1e-2\n",
    "    flux_rel_sq_jump_dist = flux_rel_sq_jump_dist_tol + 1   # make sure flux_rel_sq_jump_dist is initially greater than tolerance\n",
    "    \n",
    "    u_sq_jump_dist_prev = torch.tensor(1e-10, device=device)\n",
    "    u_rel_sq_jump_dist_tol = 1e-2\n",
    "    u_rel_sq_jump_dist = u_rel_sq_jump_dist_tol + 1   # make sure u_rel_sq_jump_dist is initially greater than tolerance\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Set upper bound on number of MH iterations\n",
    "    max_MH_iter = 500\n",
    "    \n",
    "    for MH_iter in range(max_MH_iter):\n",
    "        # Random walk Metropolis proposal for fluxes\n",
    "        flux_proposed = Normal(flux_prev, flux_proposal_sd).sample() * s_indicator\n",
    "\n",
    "        # Random walk Metropolis proposal for locations\n",
    "        u_proposed = TruncatedDiagonalMVN(u_prev, u_proposal_sd, torch.tensor(0, device=device), torch.tensor(img_dim, device=device)).sample() * s_indicator.unsqueeze(2)\n",
    "\n",
    "        # Compute acceptance probability\n",
    "        log_numerator = log_target(image, s_new, flux_proposed, u_proposed, tau_tminus1)\n",
    "        log_numerator += (TruncatedDiagonalMVN(u_proposed, u_proposal_sd, torch.tensor(0, device=device), torch.tensor(img_dim, device=device)).log_prob(u_prev) * s_indicator.unsqueeze(2)).sum(2).sum(0)\n",
    "\n",
    "        if MH_iter == 0:\n",
    "            log_denominator = log_target(image, s_prev, flux_prev, u_prev, tau_tminus1)\n",
    "            log_denominator += (TruncatedDiagonalMVN(u_prev, u_proposal_sd, torch.tensor(0, device=device), torch.tensor(img_dim, device=device)).log_prob(u_proposed) * s_indicator.unsqueeze(2)).sum(2).sum(0)\n",
    "\n",
    "        alpha = (log_numerator - log_denominator).exp().clamp(max = 1)\n",
    "        \n",
    "        # Accept proposal if prob <= alpha, reject otherwise\n",
    "        prob = Uniform(torch.zeros(num_particles), torch.ones(num_particles)).sample().to(device)\n",
    "        flux_new = flux_proposed * (prob <= alpha).unsqueeze(0) + flux_prev * (prob > alpha).unsqueeze(0)\n",
    "        u_new = u_proposed * (prob <= alpha).view(1, -1, 1) + u_prev * (prob > alpha).view(1, -1, 1)\n",
    "\n",
    "        # Compute relative squared jumping distance for fluxes\n",
    "        flux_sq_jump_dist_new_by_block = torch.stack(torch.split((flux_new - flux_tminus1)**2, num_particles//num_blocks, dim=1), dim=1).mean(2)\n",
    "        flux_sq_jump_dist_new = ((flux_sq_jump_dist_new_by_block * (flux_sq_jump_dist_new_by_block != 0)).sum()) / (flux_sq_jump_dist_new_by_block != 0).sum()\n",
    "        flux_rel_sq_jump_dist = (flux_sq_jump_dist_new - flux_sq_jump_dist_prev)/flux_sq_jump_dist_prev\n",
    "        \n",
    "        # Compute relative squared jumping distance for locations\n",
    "        u_sq_jump_dist_new_by_block = torch.stack(torch.split(((u_new - u_tminus1)**2).sum(2), num_particles//num_blocks, dim=1), dim=1).mean(2)\n",
    "        u_sq_jump_dist_new = ((u_sq_jump_dist_new_by_block * (u_sq_jump_dist_new_by_block != 0)).sum()) / (u_sq_jump_dist_new_by_block != 0).sum()\n",
    "        u_rel_sq_jump_dist = (u_sq_jump_dist_new - u_sq_jump_dist_prev)/u_sq_jump_dist_prev\n",
    "\n",
    "        # Continue loop until relative squared jumping distance falls below tolerance for flux and location \n",
    "        if flux_rel_sq_jump_dist < flux_rel_sq_jump_dist_tol and u_rel_sq_jump_dist < u_rel_sq_jump_dist_tol:\n",
    "            break\n",
    "        \n",
    "        # Cache log_denominator for next iteration\n",
    "        log_denominator = log_numerator * (prob <= alpha) + log_denominator * (prob > alpha)\n",
    "\n",
    "        # Reset fluxes and locations\n",
    "        flux_prev = flux_new\n",
    "        u_prev = u_new\n",
    "\n",
    "        flux_sq_jump_dist_prev = flux_sq_jump_dist_new\n",
    "        u_sq_jump_dist_prev = u_sq_jump_dist_new\n",
    "        \n",
    "    \n",
    "    print(\"num MH iters:\", MH_iter)\n",
    "    \n",
    "    return [flux_new, u_new]\n",
    "\n",
    "\n",
    "\n",
    "def bisection_f(image, flux_tminus1, u_tminus1, delta, ess_min):\n",
    "    log_numerator = 2*tempered_log_p_x_given_z(image, flux_tminus1, u_tminus1, delta).logsumexp(dim=1)\n",
    "    log_denominator = tempered_log_p_x_given_z(image, flux_tminus1, u_tminus1, 2*delta).logsumexp(dim=1)\n",
    "\n",
    "    return (log_numerator - log_denominator).exp() - ess_min\n",
    "\n",
    "\n",
    "\n",
    "def AdaptiveTempering(image, flux_tminus1, u_tminus1, tau_tminus1, tol, max_iter, ess_min):\n",
    "    num_blocks = flux_tminus1.size(0)\n",
    "    \n",
    "    a = torch.zeros(num_blocks, device = device)\n",
    "    b = 1 - tau_tminus1\n",
    "    c = (a+b)/2\n",
    "    \n",
    "    f_a = torch.zeros(num_blocks, device = device)\n",
    "    f_b = torch.zeros(num_blocks, device = device)\n",
    "    f_c = torch.zeros(num_blocks, device = device)\n",
    "    \n",
    "    # Compute increase in tau for every block using the bisection method\n",
    "    for j in range(max_iter):\n",
    "        if torch.all((b-a).abs() <= tol):\n",
    "            break\n",
    "\n",
    "        f_a = bisection_f(image, flux_tminus1, u_tminus1, a, ess_min)\n",
    "        f_b = bisection_f(image, flux_tminus1, u_tminus1, b, ess_min)\n",
    "        f_c = bisection_f(image, flux_tminus1, u_tminus1, c, ess_min)\n",
    "\n",
    "        a[f_a.sign() == f_c.sign()] = c[f_a.sign() == f_c.sign()]\n",
    "        b[f_b.sign() == f_c.sign()] = c[f_b.sign() == f_c.sign()]\n",
    "\n",
    "        c = (a+b)/2\n",
    "\n",
    "    # For all blocks, set the increase in tau to be the minimum increase across the blocks\n",
    "    c = c.min(0).values.repeat(num_blocks)\n",
    "    \n",
    "    return c + tau_tminus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMC(image, num_blocks, num_particles, max_iters):\n",
    "    ## Initialize\n",
    "    s_tminus1 = torch.ones(num_particles, device = device) * torch.arange(num_blocks, device = device).repeat_interleave(num_particles//num_blocks)\n",
    "    s_indicator = torch.logical_and(torch.arange(num_blocks, device = device).view(num_blocks,1) <= s_tminus1,\n",
    "                                    torch.arange(num_blocks, device = device).view(num_blocks,1) > torch.zeros(num_particles, device=device))\n",
    "    flux_tminus1 = flux_prior.sample([num_blocks, num_particles]) * s_indicator\n",
    "    u_tminus1 = u_prior.sample([num_blocks, num_particles]) * s_indicator.unsqueeze(2)\n",
    "\n",
    "    # Compute unnormalized weights for t=0 (all weights will be equal since proposal = prior)\n",
    "    log_unnormalized_weights_tminus1 = torch.zeros(num_particles, device = device)\n",
    "\n",
    "    # Compute normalized weights for t=0 (separately for s=1 particles, s=2 particles, and all particles)\n",
    "    normalized_weights_block_tminus1 = torch.stack(torch.split(log_unnormalized_weights_tminus1, num_particles//num_blocks, dim=0), dim = 0).softmax(1)\n",
    "    normalized_weights_all_tminus1 = log_unnormalized_weights_tminus1.softmax(0)\n",
    "\n",
    "    # Compute effective sample sizes for t=0 (ESS = num_particles//num_blocks since weights are all equal)\n",
    "    ess = 1/(normalized_weights_block_tminus1**2).sum(1)\n",
    "    ess_min = 0.5*num_particles//num_blocks\n",
    "\n",
    "    # Set initial tempering exponent to zero\n",
    "    tau_tminus1 = torch.zeros(num_blocks, device=device)\n",
    "    \n",
    "    # Initialize final_iter just in case SMC doesn't converge before max_iters\n",
    "    final_iter = max_iters\n",
    "    \n",
    "    for t in range(1, max_iters):        \n",
    "        ### ADAPTIVE TEMPERING\n",
    "        tau_t = AdaptiveTempering(image, flux_tminus1, u_tminus1, tau_tminus1, 1e-6, 50, ess_min)\n",
    "        \n",
    "        if t % 10 == 0:\n",
    "            print(f\"\\n======= iteration {t} =======\")\n",
    "            print(\"tau\\n\", tau_t.unique())\n",
    "        \n",
    "        ### ADAPTIVE STRATIFIED RESAMPLING\n",
    "        for block_num in range(num_blocks):\n",
    "            if ess[block_num] < ess_min:\n",
    "                bins = normalized_weights_block_tminus1[block_num,:].cumsum(0)\n",
    "                unif = (torch.arange(num_particles//num_blocks, device=device) + torch.rand(num_particles//num_blocks, device=device))/(num_particles//num_blocks)\n",
    "                resample_indices = torch.bucketize(unif, bins).clamp(min = 0, max = num_particles//num_blocks - 1) # clamp to make sure no indices are equal to num_particles//num_blocks, which would trigger device-side assert\n",
    "                \n",
    "                flux_tminus1[block_num, (block_num*num_particles//num_blocks):((block_num+1)*num_particles//num_blocks)] = torch.gather(flux_tminus1[block_num, (block_num*num_particles//num_blocks):((block_num+1)*num_particles//num_blocks)],\n",
    "                                                                                                                      0, resample_indices)\n",
    "                u_tminus1[block_num, (block_num*num_particles//num_blocks):((block_num+1)*num_particles//num_blocks), :] = torch.gather(u_tminus1[block_num, (block_num*num_particles//num_blocks):((block_num+1)*num_particles//num_blocks), :],\n",
    "                                                                                                                      0, resample_indices.unsqueeze(1).expand(num_particles//num_blocks, 2))\n",
    "\n",
    "                normalized_weights_block_tminus1[block_num,:] = (1/(num_particles//num_blocks)) * torch.ones(num_particles//num_blocks).to(device)\n",
    "                normalized_weights_all_tminus1[(block_num*num_particles//num_blocks):((block_num+1)*num_particles//num_blocks)] = (normalized_weights_all_tminus1[(block_num*num_particles//num_blocks):((block_num+1)*num_particles//num_blocks)].sum(0)/(num_particles//num_blocks)).unsqueeze(0).expand(num_particles//num_blocks)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### PROPAGATE\n",
    "        # Each particle has the same s as its parent\n",
    "        s_t = s_tminus1\n",
    "\n",
    "        updated_z = MCMC_kernel(image, s_tminus1, s_t, flux_tminus1, u_tminus1, tau_tminus1)\n",
    "        flux_t = updated_z[0]\n",
    "        u_t = updated_z[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### UPDATE WEIGHTS\n",
    "        # Compute log of incremental weights\n",
    "        log_incremental_weights_t = tempered_log_p_x_given_z(image, flux_t, u_t, tau_t - tau_tminus1).flatten(0)\n",
    "        \n",
    "        # Compute log of unnormalized weights\n",
    "        log_unnormalized_weights_t = normalized_weights_all_tminus1.log() + log_incremental_weights_t\n",
    "\n",
    "        # Compute normalized weights\n",
    "        normalized_weights_block_t = torch.stack(torch.split(log_unnormalized_weights_t, num_particles//num_blocks, dim=0), dim = 0).softmax(1).clamp(1e-40)\n",
    "        normalized_weights_all_t = log_unnormalized_weights_t.softmax(0).clamp(1e-40)\n",
    "        \n",
    "        # Compute effective sample sizes\n",
    "        ess = 1/(normalized_weights_block_t**2).sum(1)\n",
    "        \n",
    "        if torch.all(1 - tau_t.unique() < 1e-6):\n",
    "            final_iter = t\n",
    "            break\n",
    "        \n",
    "        ### Update\n",
    "        s_tminus1 = s_t\n",
    "        flux_tminus1 = flux_t\n",
    "        u_tminus1 = u_t\n",
    "        log_unnormalized_weights_tminus1 = log_unnormalized_weights_t\n",
    "        normalized_weights_block_tminus1 = normalized_weights_block_t\n",
    "        normalized_weights_all_tminus1 = normalized_weights_all_t\n",
    "        tau_tminus1 = tau_t\n",
    "    \n",
    "    return [s_t, flux_t, u_t, normalized_weights_all_t, normalized_weights_block_t, final_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce90cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Set number of particles, number of SMC steps, and sequence of tempering factors\n",
    "max_objects_smc = max_objects_generated + 2\n",
    "num_blocks = max_objects_smc + 1        # allow SMC to guess source counts in the set {0,1,2,...,max_objects_generated,max_objects_generated+1,...,max_objects_generated+2}\n",
    "num_particles = 500*num_blocks\n",
    "max_iters = 1000\n",
    "\n",
    "# Create tensors to store SMC results\n",
    "post_mean_s_smc = torch.zeros(num_images, device=device)\n",
    "prob_s_smc = torch.zeros(num_images, num_blocks, device=device)\n",
    "num_iters_smc = torch.zeros(num_images, device=device)\n",
    "reconstruction_smc = torch.zeros(num_images, H, W, device=device)\n",
    "\n",
    "# Run SMC sampler for all images\n",
    "for j in range(num_images):\n",
    "    print(f\"image {j+1} of {num_images}\")\n",
    "    smc = SMC(images[j], num_blocks, num_particles, max_iters)\n",
    "    \n",
    "    \n",
    "    post_mean_s_smc[j] = (smc[3] * smc[0]).sum()\n",
    "    prob_s_smc[j] = torch.stack(torch.split(smc[3], num_particles//num_blocks, dim=0), dim=0).sum(1)\n",
    "    num_iters_smc[j] = smc[5]\n",
    "    \n",
    "    smc_argmax_index = smc[3].argmax()\n",
    "    smc_argmax_flux = smc[1][:,smc_argmax_index]\n",
    "    smc_argmax_loc = smc[2][:,smc_argmax_index,:]\n",
    "    reconstruction_smc[j] = (psf(H, W, num_blocks, smc_argmax_loc[:,0], smc_argmax_loc[:,1], eta) * smc_argmax_flux.view(1, 1, num_blocks)).sum(2) + background_intensity\n",
    "    print(f\"\\nimage {j+1} took {num_iters_smc[j].int()} iterations.\\nfor image {j+1}, true s is {s[j]} and estimated s is {post_mean_s_smc[j]}.\\n\")\n",
    "    \n",
    "    print(f\"MSE across {j+1} images:\", ((post_mean_s_smc[:(j+1)] - s[:(j+1)])**2).mean().item())\n",
    "    print(f\"MAE across {j+1} images:\", ((post_mean_s_smc[:(j+1)] - s[:(j+1)]).abs()).mean().item())\n",
    "    print(f\"correct number of sources detected in {(post_mean_s_smc[:(j+1)].round() == s[:(j+1)]).sum()} of the {j+1} images (accuracy = {(post_mean_s_smc[:(j+1)].round() == s[:(j+1)]).sum()/(j+1)})\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb659b44",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567448b",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04573c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic images\n",
    "torch.save(s, \"results/s.pt\")\n",
    "torch.save(flux, \"results/flux.pt\")\n",
    "torch.save(u, \"results/u.pt\")\n",
    "torch.save(true_intensity, \"results/true_intensity.pt\")\n",
    "torch.save(images, \"results/images.pt\")\n",
    "\n",
    "# SMC results\n",
    "torch.save(post_mean_s_smc, \"results/post_mean_s_smc.pt\")\n",
    "torch.save(prob_s_smc, \"results/prob_s_smc.pt\")\n",
    "torch.save(reconstruction_smc, \"results/reconstruction_smc.pt\")\n",
    "torch.save(num_iters_smc, \"results/num_iters_smc.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
